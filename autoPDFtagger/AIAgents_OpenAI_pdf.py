from autoPDFtagger.AIAgents import AIAgent_OpenAI
from autoPDFtagger.AIAgents import OpenAI_model_pricelist
import logging
from autoPDFtagger.config import config
api_key = config['OPENAI-API']['API-Key']
LANGUAGE = config['DEFAULT']['language']

from autoPDFtagger.PDFDocument import PDFDocument
import json
import pprint
import re
import copy
import tiktoken

class AIAgent_OpenAI_pdf_image_analysis(AIAgent_OpenAI):
    def __init__(self):
        
        system_message = f"""
        You are a helpful assistant analyzing images inside of documents. 
        Based on the shown images, provide the following information:\n
        1. Creation date of the document.\n
        2. A short title of 3-4 words.\n
        3. A short description of 3-4 sentences.\n
        4. Suitable keywords/tags related to the content.\n
        5. Rate the importance of the document on a scale from 0 (unimportant) to 10 (vital).\n
        6. Rate your confidence for each of the above points on a scale from 0 (no information) 
        over 5 (possibly right, but only few hints) to 10 (very sure). 
        You always answer in {LANGUAGE} language.  For gathering information, 
        you use the given filename, pathname and ocr-analyzed text. You always 
        answer in a specified JSON-Format which is given in the question. """
        
        super().__init__(model="gpt-4-vision-preview", system_message=system_message)

        self.response_format="json_object"
        
    def send_image_request(self, document: PDFDocument, list_images_base64):
        logging.info("Asking GPT-Vision for analysis of " + str(len(list_images_base64)) + " Images found in " + document.get_absolute_path())
        
        user_message = (
            "Analyze following Images which are found in a document. "
            "Please extend the existing information by keeping their JSON-Format: "
            + document.to_api_json() + 
            " Try to imagine as many valuable keywords and categories as possible. "
            "Imagine additional keywords thinking of a wider context and possible categories in an archive system. "
            "Answer in JSON-Format corresponding to given input."
        )

        message_content = [
        {
            "type": "text",
            "text": user_message
        }]

        # Einzelne Bilder der Nachricht zufügen
        for base64_image in list_images_base64:
            image_content = {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{base64_image}"
                }
            }
            message_content.append(image_content)

        self.add_message(message_content)


        try:
            response = super().send_request(temperature=0.2, response_format = None, )
            return response
        except Exception as e:
            logging.error("API-Call failed")
            logging.error(e)
            return None
        
    def analyze_images(self, pdf_document: PDFDocument):
        working_doc = copy.deepcopy(pdf_document)
        
        # No scanned document
        if pdf_document.image_coverage < 100: 
            logging.info("Analyzing smaller Images")
            # Wir sortieren alle Bilder mit einer bestimmten Mindestgröße und 
            # fangen von oben an, diese in 5er Gruppen in einzelnen Anfragen 
            # an GPT-Vision zu schicken, so lange, bis entweder alle Bilder
            # analysiert sind oder has_sufficient_information true ergibt
            working_doc = self.process_images_by_size(working_doc)

        # Scanned document
        if pdf_document.image_coverage >= 100: 
            # Hier alle Seiten als Bild GTP-Vision vorlegen, welche weniger als 100 Wörter enthalten
            logging.info("Recognizing scanned document")
            working_doc = self.process_images_by_page(working_doc)

        return working_doc.to_api_json()

    def process_images_by_size(self, pdf_document: PDFDocument):
        # Create a list of all images from each page
        all_images = [image for page in pdf_document.images for image in page]
        # Sort images by pixel count (width x height)
        sorted_images = sorted(all_images, key=lambda img: img["original_width"] * img["original_height"], reverse=True)

        # Filter out images smaller than 90000 pixels (e.g., less than 300x300)
        relevant_images = [img for img in sorted_images if img["original_width"] * img["original_height"] >= 90000]

        # Process images in groups of 3
        for i in range(0, len(relevant_images), 3):
            group = relevant_images[i:i + 3]
            base64_group = []

            for image in group:
                base64_group.append(pdf_document.get_png_image_base64_by_xref(image['xref']))

            # Call ai_analyze_images with the group of images
            response = self.send_image_request(pdf_document, base64_group)
            try:
                pdf_document.set_from_json(response)
            except Exception as e:
                logging.error("API-Call for image analysis failed")
                logging.error(e)

            if pdf_document.has_sufficient_information():
                logging.info("Document information sufficient. Proceeding with next document.")
                return pdf_document
            else:
                logging.info("Still lacking information, looking for more images")
        logging.info("No more images found.")
        return pdf_document
            
    def process_images_by_page(self, pdf_document: PDFDocument):
        for page in pdf_document.pages:
            logging.debug(f"Checking Page {page['page_number']} looking for largest image")
            # Überspringe die Seite, wenn keine Bilder vorhanden sind
            # oder ausreichend Wörter vorhanden sind
            if 'max_img_xref' not in page or not page['max_img_xref'] or page['words_count'] > 100:
                logging.debug("Page not analyzed: " + str(page['words_count']) + " words")
                continue

            # Hole das größte Bild der Seite
            image_base64 = pdf_document.get_png_image_base64_by_xref(page['max_img_xref'])

            # Übergib das Bild zur Analyse
            logging.info("Asking AI for analyzing scanned page")
            response = self.send_image_request(pdf_document, [image_base64])
          
            try:
                pdf_document.set_from_json(response)
            except Exception as e:
                logging.error("API-Call for image analysis failed")
                logging.error(e)

            # Überprüfe, ob ausreichend Informationen vorliegen
            if pdf_document.has_sufficient_information():
                logging.info("Document information sufficient, proceeding.")
                return pdf_document
            else:
                logging.info("Still lacking information, looking for more pages")
        logging.info("No more pages available.")
        return pdf_document

    def add_message_with_images(self, text, list_images, role="user"):
        message_content = [
        {
            "type": "text",
            "text": text,
        }
        ]

        # Einzelne Bilder der Nachricht zufügen
        for base64_image in list_images:
            image_content = {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{base64_image}"
                }
            }
            message_content.append(image_content)


        self.messages.append({"role": role, "content": message_content})

class AIAgent_OpenAI_pdf_text_analysis(AIAgent_OpenAI):
    def __init__(self):
        
        system_message = (
            "You are a helpful assistant analyzing OCR outputs. It's important "
            "to remember that these outputs may represent only a part of the document. "
        
            "Provide the following information:\n"
            "1. Creation date of the document.\n"
            "2. A short title of 3-4 words.\n"
            "3. A meaningful description of 3-4 sentences.\n"
            "4. Suitable keywords/tags related to the content.\n"
            "5. Rate the importance of the document on a scale from 0 (unimportant) to "
            "10 (vital).\n"
            "6. Rate your confidence for each of the above points on a scale "
            "from 0 (no information, text not readable) over 5 (possibly right, but only "
            "few hints about the content of the whole document) to 10 (very sure). "
            "You always answer in {LANGUAGE} language. For gathering information, "
            "you use the given filename, pathname and OCR-analyzed text. "
            "If you are seeing a blank document, your title-confidence is alway 0."
            "You always answer in a specified JSON-Format like in this example:\n"
            "{\n"
            "    'description': '[description]',\n"
            "    'description_confidence': [number],\n"
            "    'title': '[title]',\n"
            "    'title_confidence': [number],\n"
            "    'creation_date': '[Date YY-mm-dd]',\n"
            "    'creation_date_confidence': [number],\n"
            "    'tags': ['[tag 1]', '[tag 2]', ...],\n"
            "    'tags_confidence': [[confidence tag 1], [confidence tag 2]],\n"
            "    'importance': [number],\n"
            "    'importance_confidence': [number]\n"
            "}"
        )



        super().__init__(model="gpt-4-1106-preview", system_message=system_message)

        self.response_format="json_object"
        
    def analyze_text(self, pdf_document: PDFDocument):
       
        # Step 1: Analyze the number of potentially meaningful words
        word_count = len([word for word in re.split(r'\W+', pdf_document.pdf_text) if len(word) >= 3])
        model_choice = "gpt-3.5-turbo-1106" if word_count > 20 else "gpt-4-1106-preview"
        #model_choice = "gpt-4-1106-preview" # for test purposes
        
        self.log_file = "api.log"

        logging.debug("Opting for " + model_choice)
        self.set_model(model_choice)
        
        message = ("Analyze following OCR-Output. Try to imagine as many valuable keywords and categories as possible. "
            "Imagine additional keywords thinking of a wider context and possible categories in an archive system. "
            f"Use {LANGUAGE} Language. Answer in the given pattern (JSON): "
            + pdf_document.get_short_description()
        )
        
        # Optimize request data
        request_test_str = pprint.pformat(self.messages)
        num_tokens = num_tokens_from_string(request_test_str)
        tokens_required = num_tokens + 500 # estimate 500 Tokens for answer
        
        # max tokens stored in price list table
        diff_to_max =  tokens_required - OpenAI_model_pricelist[self.model][2]
        if diff_to_max > 0: # message too long > we need to shorten it
            message = message[:-(diff_to_max*3)] # estimating 3 characters per Token

        self.add_message(message, role="user")
    
        primary_response = super().send_request(temperature=0.7, response_format = self.response_format)
        return primary_response
 
        

        # self.add_message(primary_response, role="assistant")

        # message2 = """
        #    Critically review and correct your answer. Ensure that the confidence level accurately 
        #    reflects how well the content of the original document can be estimated in terms of accuracy 
        #    and completeness based on the available text excerpts. Additionally, think of more 
        #    keywords/tags that could improve the document's findability. Be creative, but only 
        #    give useful suggestions! Respond in german language. Respond as usual in the specified JSON 
        #    format."""
        #self.add_message(message2, "user")

        #try:
        #    secondary_response = super().send_request(temperature=0.7, response_format = self.response_format)
        #except Exception as e:
        #    logging.error(e)
        #    return None
        
        #return secondary_response


class AIAgent_OpenAI_pdf_tag_analysis(AIAgent_OpenAI):
    def __init__(self):
        
        system_message = """You are a helpful assistant organizing tags. Please perform the following tasks:
        1. Correct any spelling errors in tags.
        2. Remove meaningless and irrelevant tags like 'Date', '23', 'Persons', 'Positions'
        3. Try to maintain a good specificity of the tags. Don't over-simplify.
        4. You keep the language, e.g. german tags are replaced by german tags
        Respond in JSON format with a list of replacements:
        5. Replace synonym tags (e.g. buddys and friend) with a common tag-name
        {
            "replacements": [
                {"original": "tag1", "replacement": "tag2"},
                ...
            ]
        }
        """

        super().__init__(model="gpt-4-1106-preview", system_message=system_message)

        self.response_format="json_object"
        
    def send_request(self, tags):
        self.log_file = "api.log"
        # Step 1: Simplify and summarize tags
        message = f"Improve the following tags: {tags}"
        
        self.add_message(message, role="user")

        response = super().send_request(temperature=0.3, response_format = self.response_format)
        
        message2 = "Reevaluate your response. Ensure that only synonymous tags are consolidated and that no information is lost in the process. Try to find the most specific/useful name for each tag. Preserve the original language. Check if meaningful tags are preserved and the response is complete and correct. Respond in the same JSON format."
        self.add_message(response, "assistant")
        self.add_message(message2, "user")

        logging.debug("Optimizing response...")
        response = response = super().send_request(temperature=0.3, response_format = self.response_format)
        
        try: 
            replacements = json.loads(response)
            replacements = replacements['replacements']
        except Exception as e: 
            logging.error("Could not interpret AI answer for tag simplification: " + pprint.pformat(response))
            replacements = {}

        return replacements

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens